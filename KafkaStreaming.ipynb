{"nbformat_minor": 1, "cells": [{"source": "## To use this notebook\n\nJupyter Notebooks allow you to modify and run the code in this document. To run a section (known as a 'cell',) select it and then use CTRL + ENTER, or select the play button on the toolbar above. Note that each section already has some example output beneath it, so you can see what the results of running a cell will look like.\n\nNOTE: You must run each cell in order, from top to bottom. Running cells out of order can result in an error.", "cell_type": "markdown", "metadata": {}}, {"source": "## Create the Kafka topic\n\nThis notebook uses a Kafka topic named `sparktest`. The following cell will create this topic on the Kafka cluster.\n\nNOTE: You must replace `YOUR_KAFKA_ZOOKEEPER_HOSTS` in the following cell with the Zookeeper information for your Kafka cluster. See https://github.com/Azure-Samples/hdinsight-spark-scala-kafka for information on how to get this value from the Kafka cluster.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%sh\nKAFKAZKHOSTS=YOUR_KAFKA_ZOOKEEPER_HOSTS\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 2 --partitions 8 --topic sparktest --zookeeper $KAFKAZKHOSTS", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Created topic \"sparktest\".\n"}], "metadata": {"collapsed": false}}, {"source": "## Load the Kafka streaming package\n\nThe following cell loads the Kafka streaming package for Spark from the Maven repository. Please note the version number (`1.6.2`) at the end of the second line; this must match the version of Spark that you are using. Currently it is set to Spark version 1.6.2.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "%%configure\n{ \"conf\": {\"spark.jars.packages\": \"org.apache.spark:spark-streaming-kafka_2.10:1.6.2\" }}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-streaming-kafka_2.10:1.6.2'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "## Import classes\n\nThe following cell imports classes that are used in this example. The primary ones required to use Spark streaming with Kafka are those in the `org.apache.spark.streaming` and `org.apache.kafka.clients` namespce.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "import java.util.HashMap\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka._\nimport org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\nimport scala.util.Random", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1478787730229_0004</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-larryf.b05shnvcw4yejl0muxgnat3wic.ex.internal.cloudapp.net:8088/proxy/application_1478787730229_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.24:30060/node/containerlogs/container_1478787730229_0004_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkContext available as 'sc'.\nHiveContext available as 'sqlContext'.\nimport scala.util.Random"}], "metadata": {"collapsed": false}}, {"source": "## Kafka configuration\n\nIn the following cell, replace the value of the `kafkaZkHosts` and `kafkaBrokers` with the Zookeeper hosts and Kafka broker values for your Kafka on HDInsight cluster. See [https://github.com/Azure-Samples/hdinsight-spark-scala-kafka](https://github.com/Azure-Samples/hdinsight-spark-scala-kafka) for information on how to get these values from the Kafka cluster.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "// The Kafka topic used to write, and then read from\nval topic=\"sparktest\"\n// The Zookeeper hosts for the Kafka cluster. This is used when reading from Kafka\nval kafkaZkHosts=\"###ADD YOUR KAFKA ZOOKEEPER HOST INFO HERE###\"\n// The Kafka broker hosts for the Kafka cluster\nval kafkaBrokers=\"###ADD YOUR KAFKA BROKERS INFO HERE###\"\n// The consumer group used when reading from kafka\nval group=\"mygroup\"\n// Create a map containing the topic name and how many consumer threads to create when reading\nval topicMap = Map(topic -> 5)\n// The batching interval when reading from Kafka\nval batchInterval = 2\n\n// The number of messages to write to the Kafka topic\nval numMsgs = 1000\n// Sentences that will be randomly written to Kafka\nval sentences: List[String] = List(\n        \"the cow jumped over the moon\", \n        \"an apple a day keeps the doctor away\", \n        \"four score and seven years ago\", \n        \"snow white and the seven dwarfs\", \n        \"i am at two with nature\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "sentences: List[String] = List(the cow jumped over the moon, an apple a day keeps the doctor away, four score and seven years ago, snow white and the seven dwarfs, i am at two with nature)"}], "metadata": {"collapsed": false}}, {"source": "## Create a StreamingContext\n\nThe following cell creates a new `StreamingContext` that reads a batch of messages from Kafka, breaks each into individual words, and then counts the words. The result is saved to a temporary table.\n\nNote that this runs as a background process, however it will timeout after `batchInterval * 5 * 1000` seconds. If you want to stop it before the timeout, you can use `StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }`.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "// A function that creates a streaming context\ndef createStreamingContext(): StreamingContext = {\n    // Create a new StreamingContext from the default context.\n    val ssc = new StreamingContext(sc, Seconds(batchInterval))\n    // Create the stream from the Kafka topic\n    val messageStream = KafkaUtils.createStream(ssc, kafkaZkHosts, group, topicMap).map(_._2)\n    // Split the data on space to extract the words\n    val wordStream = messageStream.flatMap(_.split(\" \"))\n    // A function to update/store the count for each word\n    val updateCount = (values: Seq[Int], state: Option[Int]) => {\n        val current = values.sum\n        val previous = state.getOrElse(0)\n        Some(current + previous)\n    }\n    // Get a running count\n    val runningCountStream = wordStream.map { x => (x, 1) }.updateStateByKey(updateCount)\n    // Save the data to a temporary table\n    runningCountStream.foreachRDD { rdd => \n                                  sqlContext.createDataFrame(rdd).toDF(\"word\", \"count\").registerTempTable(\"wordcount\")\n                                  rdd.take(1)\n                                  }\n    // Tell the stream to keep the data around for a minute, so it's there when we query later\n    ssc.remember(Minutes(1))\n    // Checkpoint for fault-tolerance\n    ssc.checkpoint(\"/\")\n    // Return the StreamingContext\n    ssc\n}\n\n// Stop any existing StreamingContext \nval stopActiveContext = true\nif (stopActiveContext) {    \n  StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }\n} \n\n// Get or create a StreamingContext\nval ssc = StreamingContext.getActiveOrCreate(createStreamingContext)\n\n// This starts the StreamingContext in the background. \nssc.start()\n\n// Set the stream to run with a timeout of batchInterval * 5 * 1000 seconds\nssc.awaitTerminationOrTimeout(batchInterval * 5 * 1000)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res22: Boolean = false"}], "metadata": {"collapsed": false}}, {"source": "## Send messages to Kafka\n\nNow that the stream to count words is started, use the next cell to write some random sentences out to Kafka. These will be picked up by the stream started in the previous cell and counts are logged to the `batch_word_count` temporary table.", "cell_type": "markdown", "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "// Configure the producer properties - these are used to write to Kafka\nval props = new HashMap[String, Object]()\n// Set the broker hosts\nprops.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaBrokers)\n// Configure the serializers\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\n  \"org.apache.kafka.common.serialization.StringSerializer\")\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\n  \"org.apache.kafka.common.serialization.StringSerializer\")\n// Create the producer\nval producer = new KafkaProducer[String, String](props)\n\n// Send numMsgs to the Kafka topic\n(1 to numMsgs).foreach { messageNum => \n    // Randomly pick a sentence\n    val sentence = Random.shuffle(sentences).take(1)(0)\n    // Create the record\n    val message = new ProducerRecord[String, String](topic, null, sentence)\n    // Send the item to the topic\n    producer.send(message)\n}", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## View the word counts\n\nFinally, the following cell selects the count of words from the temporary table.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": 7, "cell_type": "code", "source": "%%sql\nselect * from wordcount", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "widgets": {"state": {"9d42bda32010422d82aa0d0ca4f812ba": {"views": [{"cell_index": 14}]}, "45f00a968c5042df8a738920fe696d66": {"views": [{"cell_index": 14}]}}, "version": "1.2.0"}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}, "anaconda-cloud": {}}}