{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To use this notebook\n",
    "\n",
    "Jupyter Notebooks allow you to modify and run the code in this document. To run a section (known as a 'cell',) select it and then use CTRL + ENTER, or select the play button on the toolbar above. Note that each section already has some example output beneath it, so you can see what the results of running a cell will look like.\n",
    "\n",
    "NOTE: You must run each cell in order, from top to bottom. Running cells out of order can result in an error.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* An Azure Virtual Network\n",
    "* A Spark on HDInsight 3.6 cluster, inside the virtual network\n",
    "* A Kafka on HDInsight cluster, inside the virtual network\n",
    "\n",
    "## Load packages\n",
    "\n",
    "Run the next cell to load the packages required to read from Twitter and write to Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"org.apache.spark:spark-streaming_2.11:2.1.0,org.apache.bahir:spark-streaming-twitter_2.11:2.1.0,org.apache.spark:spark-streaming-kafka-0-8_2.10:2.1.0,com.google.code.gson:gson:2.4\",\n",
    "        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Kafka topic\n",
    "\n",
    "In the next cell, you must provide the Zookeeper host information for your Kafka cluster. Use the following steps to get this information:\n",
    "\n",
    "* From __Bash__ or other Unix shell:\n",
    "\n",
    "    ```bash\n",
    "CLUSTERNAME='the name of your HDInsight cluster'\n",
    "PASSWORD='the password for your cluster login account'\n",
    "curl -u admin:$PASSWORD -G \"https://$CLUSTERNAME.azurehdinsight.net/api/v1/clusters/$CLUSTERNAME/services/ZOOKEEPER/components/ZOOKEEPER_SERVER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):2181\"] | join(\",\")' | cut -d',' -f1,2\n",
    "    ```\n",
    "\n",
    "* From __Azure PowerShell__:\n",
    "\n",
    "    ```powershell\n",
    "$creds = Get-Credential -UserName \"admin\" -Message \"Enter the HDInsight login\"\n",
    "$clusterName = Read-Host -Prompt \"Enter the Kafka cluster name\"\n",
    "$resp = Invoke-WebRequest -Uri \"https://$clusterName.azurehdinsight.net/api/v1/clusters/$clusterName/services/ZOOKEEPER/components/ZOOKEEPER_SERVER\" `\n",
    "    -Credential $creds\n",
    "$respObj = ConvertFrom-Json $resp.Content\n",
    "$brokerHosts = $respObj.host_components.HostRoles.host_name[0..1]\n",
    "($brokerHosts -join \":2181,\") + \":2181\"\n",
    "    ````\n",
    "\n",
    "The return value is similar to the following example:\n",
    "\n",
    "`zk0-kafka.ztgnbfvxu2mudoa5h5zzc1uncg.cx.internal.cloudapp.net:2181,zk1-kafka.ztgnbfvxu2mudoa5h5zzc1uncg.cx.internal.cloudapp.net:2181`\n",
    "\n",
    "Replace the `YOUR_ZOOKEEPER_HOSTS` in the next cell with the returned value, and then run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic tweets --zookeeper 'YOUR_ZOOKEEPER_HOSTS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and configuration\n",
    "\n",
    "In the next cell, you must provide configuration information for a __Twitter app__ and your __Kafka brokers__.\n",
    "\n",
    "1. To create a Twitter app, see [https://apps.twitter.com](https://apps.twitter.com). After creating an app, add the __consumer key__, __consumer secret__, __access token__, and __access token secret__ in the next cell.\n",
    "\n",
    "2. Change the value of `kafkaBrokers` to the Kafka broker hosts for your Kafka cluster. The value should be a comma-delimited list of the hosts, similar to the following example:\n",
    "\n",
    "        wn0-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092,wn1-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092\n",
    "        \n",
    "    To find the Kafka brokers information for your Kafka on HDInsight cluster, you can use the Ambari REST API. The following examples demonstrate how to retrieve this information using the the `curl` and `jq` utilities (from Bash) or Windows PowerShell:\n",
    "\n",
    "    * From __Bash__ or other Unix shell:\n",
    "\n",
    "        ```bash\n",
    "CLUSTERNAME='the name of your HDInsight cluster'\n",
    "PASSWORD='the password for your cluster login account'\n",
    "curl -u admin:$PASSWORD -G \"https://$CLUSTERNAME.azurehdinsight.net/api/v1/clusters/$CLUSTERNAME/services/KAFKA/components/KAFKA_BROKER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):9092\"] | join(\",\")' | cut -d',' -f1,2\n",
    "        ```\n",
    "\n",
    "    * From __Azure PowerShell__:\n",
    "\n",
    "    ```powershell\n",
    "$creds = Get-Credential -UserName \"admin\" -Message \"Enter the HDInsight login\"\n",
    "$clusterName = Read-Host -Prompt \"Enter the Kafka cluster name\"\n",
    "$resp = Invoke-WebRequest -Uri \"https://$clusterName.azurehdinsight.net/api/v1/clusters/$clusterName/services/KAFKA/components/KAFKA_BROKER\" `\n",
    "    -Credential $creds\n",
    "$respObj = ConvertFrom-Json $resp.Content\n",
    "$brokerHosts = $respObj.host_components.HostRoles.host_name[0..1]\n",
    "($brokerHosts -join \":9092,\") + \":9092\"\n",
    "    ```\n",
    "\n",
    "   Note that you only need one or two entries for broker and Zookeeper hosts. The previous examples return two hosts.\n",
    "\n",
    "3. Run the next cell to configure Twitter and Kafka for this notebook.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.twitter._\n",
    "import org.apache.spark.streaming.kafka._\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "import java.util.HashMap\n",
    "import com.google.gson.Gson\n",
    "\n",
    "// Twitter configuration\n",
    "val consumerKey=\"replace with your consumer key\"\n",
    "val consumerSecret=\"replace with your consumer secret\"\n",
    "val accessToken=\"replace with your access token\"\n",
    "val accessTokenSecret=\"replace with your access token secret\"\n",
    "\n",
    "//Words that we want to filter tweets for.\n",
    "//Note: You want to use words that are used fairly often on Twitter, otherwise you\n",
    "//      will not capture many (or any) tweets.\n",
    "val filters=Array(\"coffee\",\"hadoop\",\"spark\",\"kafka\",\"xbox\",\"ps4\",\"nintendo\")\n",
    "\n",
    "// Kafka configuration\n",
    "// kafkaBrokers should contain a comma-delimited list of brokers. For example:\n",
    "// kafkaBrokers = \"wn0-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092,wn1-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092,wn2-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092\"\n",
    "val kafkaBrokers=\"your Kafka brokers\"\n",
    "val kafkaTopic=\"tweets\"\n",
    "\n",
    "// Make the Twitter config visible to Twitter4j\n",
    "System.setProperty(\"twitter4j.oauth.consumerKey\", consumerKey)\n",
    "System.setProperty(\"twitter4j.oauth.consumerSecret\", consumerSecret)\n",
    "System.setProperty(\"twitter4j.oauth.accessToken\", accessToken)\n",
    "System.setProperty(\"twitter4j.oauth.accessTokenSecret\", accessTokenSecret)\n",
    "\n",
    "println(\"Finished configuring Twitter client\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Start the stream\n",
    "\n",
    "Run the next cell to begin streaming tweets into Kafka. This stream will run for a minute.\n",
    "\n",
    "IMPORTANT: If the output of the cell is \"0 tweets to kafka\", then you may have an incorrect Twitter access token or consumer secret. Or the words you are subscribing to may not be popular on Twitter at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Create an accumulator so we can track the number of tweets emitted to Kafka\n",
    "val numTweets = sc.accumulator(0L,\"Tweets sent to Kafka\")\n",
    "\n",
    "// The streaming context (DStream) for reading from Twitter and writing to Kafka\n",
    "def createStreamingContext(): StreamingContext = {\n",
    "    // Create the Kafka producer\n",
    "    val producerProperties = new HashMap[String, Object]()\n",
    "    producerProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaBrokers)\n",
    "    producerProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\n",
    "                           \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    producerProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\n",
    "                           \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    \n",
    "    // set up the streaming context\n",
    "    val ssc = new StreamingContext(sc, Seconds(5))\n",
    "    // set up the stream, which we just convert to JSON\n",
    "    val stream = TwitterUtils.createStream(ssc, None, filters)\n",
    "    // Write the data to Kafka\n",
    "    stream.foreachRDD( rdd => {\n",
    "        rdd.foreachPartition( partition => {\n",
    "            val producer = new KafkaProducer[String, String](producerProperties)\n",
    "            partition.foreach( record => {\n",
    "                // Convert the data to JSON\n",
    "                val gson = new Gson()\n",
    "                val data = gson.toJson(record)\n",
    "                val message = new ProducerRecord[String, String](kafkaTopic, null, data)\n",
    "                // Send the tweet data to Kafka\n",
    "                producer.send(message)\n",
    "                // Increment the counter\n",
    "                numTweets +=1\n",
    "            })\n",
    "            producer.close()\n",
    "        })\n",
    "    })\n",
    "    ssc\n",
    "}\n",
    "\n",
    "val ssc = StreamingContext.getActiveOrCreate(createStreamingContext)\n",
    "ssc.start()\n",
    "// Timeout after 60 seconds\n",
    "ssc.awaitTerminationOrTimeout(60000)\n",
    "println(\"Finished writting \" + numTweets + \" tweets to Kafka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
