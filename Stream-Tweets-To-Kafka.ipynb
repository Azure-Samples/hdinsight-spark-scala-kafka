{"nbformat_minor": 2, "cells": [{"source": "## To use this notebook\n\nJupyter Notebooks allow you to modify and run the code in this document. To run a section (known as a 'cell',) select it and then use CTRL + ENTER, or select the play button on the toolbar above. Note that each section already has some example output beneath it, so you can see what the results of running a cell will look like.\n\nNOTE: You must run each cell in order, from top to bottom. Running cells out of order can result in an error.\n\n## Requirements\n\n* An Azure Virtual Network\n* A Spark on HDInsight 3.6 cluster, inside the virtual network\n* A Kafka on HDInsight cluster, inside the virtual network\n\n## Load packages\n\nRun the next cell to load the packages required to read from Twitter and write to Kafka.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-streaming_2.11:2.1.0,org.apache.bahir:spark-streaming-twitter_2.11:2.1.0,org.apache.spark:spark-streaming-kafka-0-8_2.10:2.1.0,com.google.code.gson:gson:2.4\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n    }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-streaming_2.11:2.1.0,org.apache.bahir:spark-streaming-twitter_2.11:2.1.0,org.apache.spark:spark-streaming-kafka-0-8_2.10:2.1.0,com.google.code.gson:gson:2.4', u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1498682828153_0006</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-spark.2glmamezcpsevgtombx3ulcgih.ex.internal.cloudapp.net:8088/proxy/application_1498682828153_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.12:30060/node/containerlogs/container_1498682828153_0006_01_000001/livy\">Link</a></td><td></td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "## Setup and configuration\n\nIn the next cell, you must provide configuration information for a __Twitter app__ and your __Kafka brokers__.\n\n1. To create a Twitter app, see [https://apps.twitter.com](https://apps.twitter.com). After creating an app, add the __consumer key__, __consumer secret__, __access token__, and __access token secret__ in the next cell.\n\n2. Change the value of `kafkaBrokers` to the Kafka broker hosts for your Kafka cluster. The value should be a comma-delimited list of the hosts, similar to the following example:\n\n        wn0-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092,wn1-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092,wn2-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092\n        \n    To find the Kafka brokers information for your Kafka on HDInsight cluster, you can use the Ambari REST API. The following examples demonstrate how to retrieve this information using the the `curl` and `jq` utilities (from Bash) or Windows PowerShell:\n\n    * From __Bash__ or other Unix shell:\n\n        ```bash\nCLUSTERNAME='the name of your HDInsight cluster'\nPASSWORD='the password for your cluster login account'\ncurl -u admin:$PASSWORD -G \"https://$CLUSTERNAME.azurehdinsight.net/api/v1/clusters/$CLUSTERNAME/services/KAFKA/components/KAFKA_BROKER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):9092\"] | join(\",\")'\n        ```\n\n        * From __Azure Powershell__:\n\n        ```powershell\n$creds = Get-Credential -UserName \"admin\" -Message \"Enter the HDInsight login\"\n$clusterName = Read-Host -Prompt \"Enter the Kafka cluster name\"\n$resp = Invoke-WebRequest -Uri \"https://$clusterName.azurehdinsight.net/api/v1/clusters/$clusterName/services/KAFKA/components/KAFKA_BROKER\" `\n    -Credential $creds\n$respObj = ConvertFrom-Json $resp.Content\n$brokerHosts = $respObj.host_components.HostRoles.host_name\n($brokerHosts -join \":9092,\") + \":9092\"\n        ```\n\n3. Run the next cell to configure Twitter and Kafka for this notebook.\n    ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "import org.apache.spark.streaming._\nimport org.apache.spark.streaming.twitter._\nimport org.apache.spark.streaming.kafka._\nimport org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\nimport java.util.HashMap\nimport com.google.gson.Gson\n\n// Twitter configuration\nval consumerKey=\"replace with your consumer key\"\nval consumerSecret=\"replace with your consumer secret\"\nval accessToken=\"replace with your access token\"\nval accessTokenSecret=\"replace with your access token secret\"\n\n//Words that we want to filter tweets for.\n//Note: You want to use words that are used fairly often on Twitter, otherwise you\n//      will not capture many (or any) tweets.\nval filters=Array(\"coffee\",\"hadoop\",\"spark\",\"kafka\",\"xbox\",\"ps4\",\"nintendo\")\n\n// Kafka configuration\n// kafkaBrokers should contain a comma-delimited list of brokers. For example:\n// kafkaBrokers = \"wn0-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092,wn1-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092,wn2-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092\"\nval kafkaBrokers=\"your Kafka brokers\"\nval kafkaTopic=\"tweets\"\n\n// Make the Twitter config visible to Twitter4j\nSystem.setProperty(\"twitter4j.oauth.consumerKey\", consumerKey)\nSystem.setProperty(\"twitter4j.oauth.consumerSecret\", consumerSecret)\nSystem.setProperty(\"twitter4j.oauth.accessToken\", accessToken)\nSystem.setProperty(\"twitter4j.oauth.accessTokenSecret\", accessTokenSecret)\n\nprintln(\"Finished configuring Twitter client\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1498682828153_0007</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-spark.2glmamezcpsevgtombx3ulcgih.ex.internal.cloudapp.net:8088/proxy/application_1498682828153_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.9:30060/node/containerlogs/container_1498682828153_0007_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\nFinished configuring Twitter client"}], "metadata": {"collapsed": false}}, {"source": "## Start the stream\n\nRun the next cell to begin streaming tweets into Kafka. This stream will run for a minute.", "cell_type": "markdown", "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "// Create an accumulator so we can track the number of tweets emitted to Kafka\nval numTweets = sc.accumulator(0L,\"Tweets sent to Kafka\")\n\n// The streaming context (DStream) for reading from Twitter and writing to Kafka\ndef createStreamingContext(): StreamingContext = {\n    // Create the Kafka producer\n    val producerProperties = new HashMap[String, Object]()\n    producerProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaBrokers)\n    producerProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\n                           \"org.apache.kafka.common.serialization.StringSerializer\")\n    producerProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\n                           \"org.apache.kafka.common.serialization.StringSerializer\")\n    \n    // set up the streaming context\n    val ssc = new StreamingContext(sc, Seconds(5))\n    // set up the stream, which we just convert to JSON\n    val stream = TwitterUtils.createStream(ssc, None, filters)\n    // Write the data to Kafka\n    stream.foreachRDD( rdd => {\n        rdd.foreachPartition( partition => {\n            val producer = new KafkaProducer[String, String](producerProperties)\n            partition.foreach( record => {\n                // Convert the data to JSON\n                val gson = new Gson()\n                val data = gson.toJson(record)\n                val message = new ProducerRecord[String, String](kafkaTopic, null, data)\n                // Send the tweet data to Kafka\n                producer.send(message)\n                // Increment the counter\n                numTweets +=1\n            })\n            producer.close()\n        })\n    })\n    ssc\n}\n\nval ssc = StreamingContext.getActiveOrCreate(createStreamingContext)\nssc.start()\n// Timeout after 60 seconds\nssc.awaitTerminationOrTimeout(60000)\nprintln(\"Finished writting \" + numTweets + \" tweets to Kafka\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Finished writting 318 tweets to Kafka"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}